name: Testing Tools

on:
  workflow_dispatch:

permissions:
  contents: write

env:
  MAX_PARALLEL: 20 # <— throttle HTTPX jobs

jobs:
  prepare_matrix:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.set_matrix.outputs.matrix }}
    steps:
      - uses: actions/checkout@v3

      - name: Build chunk matrix from target.txt
        id: set_matrix
        run: |
          INPUT_FILE="target.txt"
          CHUNK_DIR="results/chunks"

          # 1. Check if the input file exists
          if [ ! -f "$INPUT_FILE" ]; then
            echo "::error::Input file '$INPUT_FILE' not found!"
            echo "matrix=[]" >> $GITHUB_OUTPUT
            exit 0
          fi

          # 2. Get the total line count and apply splitting logic
          LINES=$(wc -l < "$INPUT_FILE")
          echo "Source $INPUT_FILE has $LINES lines"
          mkdir -p "$CHUNK_DIR"

          if [ "$LINES" -ge 200000 ]; then
            echo "— ≥200 000 lines: splitting into 50 equal chunks"
            split -n l/50 \
                  --numeric-suffixes=1 --suffix-length=2 --additional-suffix=.txt \
                  "$INPUT_FILE" "$CHUNK_DIR/chunk_"
          elif [ "$LINES" -gt 1500 ]; then
            echo "— > 1500 lines: splitting into 20 equal chunks"
            split -n l/20 \
                  --numeric-suffixes=1 --suffix-length=2 --additional-suffix=.txt \
                  "$INPUT_FILE" "$CHUNK_DIR/chunk_"
          else
            echo "— ≤ 1500 lines: splitting into 200-line chunks"
            split -l 200 \
                  --numeric-suffixes=1 --suffix-length=2 --additional-suffix=.txt \
                  "$INPUT_FILE" "$CHUNK_DIR/chunk_"
          fi

          # 3. Debug: list out what we just created
          echo "Contents of $CHUNK_DIR:"
          ls -R "$CHUNK_DIR" || echo "(none found)"

          # 4. Build a JSON object for each chunk file
          pairs=()
          for CH in "$CHUNK_DIR"/chunk_*; do
            [ -f "$CH" ] || continue
            pairs+=( "{\"chunk\":\"$CH\"}" )
          done
          
          # 5. Emit the chunk-level matrix
          if [ ${#pairs[@]} -eq 0 ]; then
            echo "No chunks created. Emitting empty matrix."
            echo "matrix=[]" >> $GITHUB_OUTPUT
          else
            echo "matrix<<EOF" >> $GITHUB_OUTPUT
            printf '%s\n' "${pairs[@]}" | jq -s . >> $GITHUB_OUTPUT
            echo "EOF" >> $GITHUB_OUTPUT
          fi

      - name: Upload chunks artifact
        uses: actions/upload-artifact@v4
        with:
          name: httpx-chunks
          path: results/chunks/**

  httpx:
    needs: prepare_matrix
    runs-on: ubuntu-latest
    container:
      image: ghcr.io/pcoder7/spider-puredns-actions:latest
      credentials:
        username: ${{ github.actor }}
        password: ${{ secrets.GHCR_TOKEN }}
    if: ${{ needs.prepare_matrix.outputs.matrix != '[]' }}
    strategy:
      fail-fast: false
      matrix:
        item: ${{ fromJson(needs.prepare_matrix.outputs.matrix) }}
      max-parallel: 20

    steps:
      - name: Ensure httpx is installed
        run: |
          if ! command -v httpx >/dev/null; then
            go install -v github.com/projectdiscovery/httpx/cmd/httpx@latest
          else
            echo "httpx already installed"
          fi

      - name: Download chunk files
        uses: actions/download-artifact@v4
        with:
          name: httpx-chunks
          # FIX: Download the files INTO the directory structure the script expects.
          # This will create results/chunks/ and place the files inside it.
          path: results/chunks

      - name: List downloaded files (for debugging)
        run: |
          echo "Listing files in the workspace to confirm download location:"
          ls -R

      - name: Probe with httpx on ${{ matrix.item.chunk }}
        run: |
          CH="${{ matrix.item.chunk }}"
          # This path will now be correct because the download step recreated it.
          if [ ! -f "$CH" ]; then
            echo "::error::File not found at the expected path: $CH"
            exit 0
          fi
          
          OUT="results/httpx_out" # Simplified, common output directory
          mkdir -p "$OUT"

          LINES=$(wc -l < "$CH")
          echo "▶ Processing $CH — $LINES lines"

          if [ "$LINES" -le 1500 ]; then
            # small: single httpx run
            echo "— ≤1500 lines: single httpx with 100 threads"
            httpx -l "$CH" -silent -threads 100 \
              -o "${OUT}/httpx_$(basename "$CH")" | nuclei -t ~/nuclei-templates/http/exposed-panels/globalprotect-panel.yaml -silent
          else
            # big: split into sub-chunks and run with GNU Parallel
            TMP=$(mktemp -d)
            split -l 1500 --numeric-suffixes=1 --suffix-length=2 \
                  --additional-suffix=.txt \
                  "$CH" "$TMP/sub_"

            if [ "$LINES" -gt 35000 ]; then JOBS=12; THREADS=250
            elif [ "$LINES" -gt 15000 ]; then JOBS=10; THREADS=200
            else JOBS=8; THREADS=100
            fi

            echo "— Launching parallel -j$JOBS with $THREADS threads"
            parallel -j"$JOBS" \
              httpx -l {} -silent -threads "$THREADS" -retries 1 -random-agent -timeout 10 \
                    -o "$OUT/httpx_{/.}.txt" |  nuclei -t ~/nuclei-templates/http/exposed-panels/globalprotect-panel.yaml -silent
              ::: "$TMP"/sub_*.txt
            rm -rf "$TMP"
          fi

          # Consolidate results for this chunk (from sub-chunks if they exist)
          FINAL_CHUNK_FILE="${OUT}/httpx_$(basename "$CH")"
          
          # Find all potential result files for THIS CHUNK and consolidate them
          # This correctly handles both the single-run and parallel-run cases
          find "$OUT" -maxdepth 1 -type f -name "httpx_$(basename "$CH" .txt)*" -print0 \
            | xargs -0 --no-run-if-empty cat \
            | sort -u > "$FINAL_CHUNK_FILE.tmp" && mv "$FINAL_CHUNK_FILE.tmp" "$FINAL_CHUNK_FILE"

          # Cleanup intermediate sub-chunk results
          find "$OUT" -maxdepth 1 -type f -name 'httpx_sub_*.txt' -delete

      - name: Compute artifact details
        id: art_details
        run: |
          CHUNK_BASENAME=$(basename "${{ matrix.item.chunk }}")
          ARTIFACT_NAME="httpx-${CHUNK_BASENAME}"
          FILE_TO_UPLOAD="results/httpx_out/httpx_${CHUNK_BASENAME}"
          echo "artifact_name=${ARTIFACT_NAME}" >> $GITHUB_OUTPUT
          echo "file_to_upload=${FILE_TO_UPLOAD}" >> $GITHUB_OUTPUT

      - name: Upload per-chunk httpx results
        uses: actions/upload-artifact@v4
        with:
          name: ${{ steps.art_details.outputs.artifact_name }}
          path: ${{ steps.art_details.outputs.file_to_upload }}
          if-no-files-found: error
          retention-days: 1

  aggregate_results:
    needs: [prepare_matrix, httpx]
    runs-on: ubuntu-latest
    if: ${{ needs.prepare_matrix.outputs.matrix != '[]' }}
    steps:
      - name: Download all httpx chunk artifacts
        uses: actions/download-artifact@v4
        with:
          path: temp-results
          pattern: httpx-chunk_*
          merge-multiple: true

      - name: Aggregate all chunk results
        run: |
          FINAL_RESULT_FILE="final_httpx_results.txt"
          echo "Aggregating all chunk results into $FINAL_RESULT_FILE..."
          
          # Combine all downloaded result files, sort, and remove duplicates
          cat temp-results/* | sort -u > "$FINAL_RESULT_FILE"

          echo "Aggregation complete. Total unique findings: $(wc -l < $FINAL_RESULT_FILE)"

      - name: Upload final aggregated result
        uses: actions/upload-artifact@v4
        with:
          name: final-aggregated-result
          path: final_httpx_results.txt
          retention-days: 1
