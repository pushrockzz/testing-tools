name: Testing tools against a file

on:
  workflow_dispatch: # Allows you to run this workflow manually

permissions:
  contents: write # We only need to read the repository contents

jobs:
  prepare-chunks:
    name: Prepare Chunks and Build Matrix
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.build_matrix.outputs.matrix }}
      chunk_count: ${{ steps.build_matrix.outputs.chunk_count }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Create Chunks and Build Matrix
        id: build_matrix
        shell: bash
        run: |
          # --- CONFIGURATION ---
          # This workflow will look for a file named "target.txt" in your repo.
          INPUT_FILE="target.txt" 
          # The number of lines from your input file to include in each parallel job.
          CHUNK_SIZE=100
          # ---------------------

          CHUNK_DIR="chunks"

          if [ ! -f "$INPUT_FILE" ]; then
            echo "::error::Input file '$INPUT_FILE' not found in the repository root."
            exit 1
          fi

          mkdir -p "$CHUNK_DIR"
          echo "-> Splitting '$INPUT_FILE' into chunks of $CHUNK_SIZE lines..."
          split -l "$CHUNK_SIZE" -a 3 --numeric-suffixes=1 "$INPUT_FILE" "$CHUNK_DIR/chunk_"
          
          echo "-> Created the following chunks:"
          ls -1 "$CHUNK_DIR"

          # --- APPLYING THE SUCCESSFUL PATTERN ---
          # This is the robust, iterative method for building the JSON matrix.
          echo "-> Building job matrix..."
          JSON_MATRIX='[]'
          CHUNK_COUNT=0
          
          # This loop safely iterates through each chunk file found.
          # The -print0 and -d $'\0' handles filenames with spaces or special characters.
          while IFS= read -r -d $'\0' chunk_file; do
            # jq safely adds a new JSON object for the current file to the JSON_MATRIX variable.
            # The object will look like: {"chunk_file":"chunks/chunk_001"}
            JSON_MATRIX=$(printf '%s' "$JSON_MATRIX" | jq -c --arg file_path "$chunk_file" '. + [{chunk_file:$file_path}]')
            # Increment the counter for each chunk found.
            ((CHUNK_COUNT++))
          done < <(find "$CHUNK_DIR" -maxdepth 1 -type f -name "chunk_*" -print0)

          if [ "$CHUNK_COUNT" -eq 0 ]; then
            echo "::warning::No chunk files were found to create a matrix."
          else
            echo "-> Matrix creation complete. $CHUNK_COUNT jobs will be created."
          fi

          # Output the final, perfectly formatted JSON string and the count for the next jobs.
          echo "Final Matrix: $JSON_MATRIX"
          echo "matrix=$JSON_MATRIX" >> "$GITHUB_OUTPUT"
          echo "chunk_count=$CHUNK_COUNT" >> "$GITHUB_OUTPUT"



      - name: Upload Chunks as Artifact
        uses: actions/upload-artifact@v4
        with:
          name: nuclei-chunks
          path: chunks/
          retention-days: 1

  run-nuclei-in-parallel:
    needs: prepare-chunks
    if: ${{ needs.prepare-chunks.outputs.chunk_count > 0 }}
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        # This creates a parallel job for each file path in the matrix output from the previous job
        chunk_file: ${{ fromJson(needs.prepare-chunks.outputs.matrix) }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v3
        with:
          # Fetch full Git history so previous commits are available for comparison
          fetch-depth: 0   
          
      - name: Setup Go
        uses: actions/setup-go@v5
        with:
          go-version: '1.23'

      - name: Cache Go modules & binaries
        uses: actions/cache@v3
        with:
          path: |
            $HOME/go/pkg/mod
            ~/.cache/go-build
            $HOME/go/bin
          key: ${{ runner.os }}-go-cache-${{ github.ref_name }}
          restore-keys: |
            ${{ runner.os }}-go-cache-

      - name: Install Tools
        run: |
          # Installing httpx
          if ! command -v httpx >/dev/null; then
            echo "Installing httpxâ€¦"
            go install -v github.com/projectdiscovery/httpx/cmd/httpx@latest
          else
            echo "httpx already in cache"
          fi    

      - name: Download Chunks Artifact
        uses: actions/download-artifact@v4
        with:
          name: nuclei-chunks
          path: chunks/

      - name: Run Nhttpx on Chunk
        id: run_tool
        run: |
          CHUNK_BASENAME=$(basename "${{ matrix.chunk_file }}")
          OUTPUT_DIR="results"
          OUTPUT_FILE="$OUTPUT_DIR/result_$CHUNK_BASENAME"
          mkdir -p "$OUTPUT_DIR"

          echo "Running Nuclei on file: ${{ matrix.chunk_file }}"
          
          # --- MANUALLY CONFIGURED COMMAND ---
          # This is where you write the exact command you want to run.
          # `${{ matrix.chunk_file }}` is the placeholder for the input list.
          # `$OUTPUT_FILE` is the placeholder for the output file for this specific chunk.

          
          httpx  -l "${{ matrix.chunk_file }}" -t 100 -o "$OUTPUT_FILE" -rl 100 -random-agent -delay 200ms -silent 
          
          
          # ------------------------------------

          echo "output_file=$OUTPUT_FILE" >> $GITHUB_OUTPUT

      - name: Upload Individual Result Artifact
        uses: actions/upload-artifact@v4
        with:
          name: result-${{ matrix.chunk_file }}
          path: ${{ steps.run_tool.outputs.output_file }}
          retention-days: 1 

  aggregate-results:
    needs: run-nuclei-in-parallel
    if: always() # This ensures the aggregation job runs even if some nuclei jobs fail
    runs-on: ubuntu-latest
    steps:
      - name: Create Temporary Directory for Results
        run: mkdir -p temp-results

      - name: Download All Result Artifacts
        uses: actions/download-artifact@v4
        with:
          path: temp-results/
          pattern: result-chunks/chunk_*.txt
          merge-multiple: true

      - name: Aggregate All Results into a Single File
        id: aggregate
        run: |
          echo "Aggregating all results..."
          # Combine all downloaded result files into one.
          # The `sort -u` command will also remove any duplicate findings.
          cat temp-results/* | sort -u > nuclei-final-results.txt
          echo "Final aggregated results created at nuclei-final-results.txt"
          echo "Total unique findings: $(wc -l < nuclei-final-results.txt)"

      - name: Upload Final Aggregated Results
        uses: actions/upload-artifact@v4
        with:
          name: final-nuclei-results
          path: nuclei-final-results.txt
          retention-days: 1
