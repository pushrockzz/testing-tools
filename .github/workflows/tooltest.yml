name: Testing Tools

on:
  workflow_dispatch:

permissions:
  contents: write

env:
  MAX_PARALLEL: 20 # <— throttle HTTPX jobs

jobs:
  prepare_matrix:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.set_matrix.outputs.matrix }}
    steps:
      - uses: actions/checkout@v3

      - name: Build chunk matrix from target.txt
        id: set_matrix
        run: |
          INPUT_FILE="target.txt"
          CHUNK_DIR="results/chunks"

          # 1. Check if the input file exists
          if [ ! -f "$INPUT_FILE" ]; then
            echo "::error::Input file '$INPUT_FILE' not found!"
            echo "matrix=[]" >> $GITHUB_OUTPUT
            exit 0
          fi

          # 2. Get the total line count and apply splitting logic
          LINES=$(wc -l < "$INPUT_FILE")
          echo "Source $INPUT_FILE has $LINES lines"
          mkdir -p "$CHUNK_DIR"

          if [ "$LINES" -ge 200000 ]; then
            echo "— ≥200 000 lines: splitting into 50 equal chunks"
            split -n l/50 \
                  --numeric-suffixes=1 --suffix-length=2 --additional-suffix=.txt \
                  "$INPUT_FILE" "$CHUNK_DIR/chunk_"
          elif [ "$LINES" -gt 1500 ]; then
            echo "— > 1500 lines: splitting into 20 equal chunks"
            split -n l/20 \
                  --numeric-suffixes=1 --suffix-length=2 --additional-suffix=.txt \
                  "$INPUT_FILE" "$CHUNK_DIR/chunk_"
          else
            echo "— ≤ 1500 lines: splitting into 200-line chunks"
            split -l 200 \
                  --numeric-suffixes=1 --suffix-length=2 --additional-suffix=.txt \
                  "$INPUT_FILE" "$CHUNK_DIR/chunk_"
          fi

          # 3. Debug: list out what we just created
          echo "Contents of $CHUNK_DIR:"
          ls -R "$CHUNK_DIR" || echo "(none found)"

          # 4. Build a JSON object for each chunk file
          pairs=()
          for CH in "$CHUNK_DIR"/chunk_*; do
            [ -f "$CH" ] || continue
            pairs+=( "{\"chunk\":\"$CH\"}" )
          done
          
          # 5. Emit the chunk-level matrix
          if [ ${#pairs[@]} -eq 0 ]; then
            echo "No chunks created. Emitting empty matrix."
            echo "matrix=[]" >> $GITHUB_OUTPUT
          else
            echo "matrix<<EOF" >> $GITHUB_OUTPUT
            printf '%s\n' "${pairs[@]}" | jq -s . >> $GITHUB_OUTPUT
            echo "EOF" >> $GITHUB_OUTPUT
          fi

      - name: Upload chunks artifact
        uses: actions/upload-artifact@v4
        with:
          name: httpx-chunks
          path: results/chunks/**
          retention-days: 1

  httpx:
    needs: prepare_matrix
    runs-on: ubuntu-latest
    container:
      image: ghcr.io/pcoder7/spider-puredns-actions:latest
      credentials:
        username: ${{ github.actor }}
        password: ${{ secrets.GHCR_TOKEN }}
    if: ${{ needs.prepare_matrix.outputs.matrix != '[]' }}
    strategy:
      fail-fast: false
      matrix:
        item: ${{ fromJson(needs.prepare_matrix.outputs.matrix) }}
      max-parallel: 20

    steps:
      - name: Install dependencies and tools
        run: |
          # Install system dependencies, including gcc/g++ and build tools
          apt-get update -y
          apt-get install -y build-essential git ripgrep

          # Ensure Go tools are in PATH
          export PATH=$PATH:$(go env GOPATH)/bin

          # Install cut-cdn
          echo "Installing cut-cdn..."
          go install github.com/ImAyrix/cut-cdn@latest

          # Install hakip2host
          echo "Installing hakip2host..."
          go install github.com/hakluke/hakip2host@latest

          # Install caduceus
          echo "Installing caduceus..."
          go install github.com/g0ldencybersec/Caduceus/cmd/caduceus@latest

          # Install CloudRecon
          echo "Installing CloudRecon..."
          go install github.com/g0ldencybersec/CloudRecon@latest

          # Install httpx
          echo "Installing httpx..."
          go install -v github.com/projectdiscovery/httpx/cmd/httpx@latest

      - name: Download chunk files
        uses: actions/download-artifact@v4
        with:
          name: httpx-chunks
          # This will create results/chunks/ and place the files inside it.
          path: results/chunks

      - name: List downloaded files (for debugging)
        run: |
          echo "Listing files in the workspace to confirm download location:"
          ls -R

      - name: Process ${{ matrix.item.chunk }} with cut-cdn, hakip2host, caduceus, and httpx
        run: |
          CH_PATH="${{ matrix.item.chunk }}"
          CH_BASENAME=$(basename "$CH_PATH")
          OUT_DIR="results/processed_out"
          mkdir -p "$OUT_DIR"

          if [ ! -f "$CH_PATH" ]; then
            echo "::error::File not found at the expected path: $CH_PATH"
            exit 0
          fi
          
          echo "▶ Processing $CH_PATH"

          # 1. Filter with cut-cdn (NO CHANGES HERE)
          NO_CDN_FILE="$OUT_DIR/No-CDN_${CH_BASENAME}"
          echo "Running cut-cdn on $CH_PATH..."
          cat "$CH_PATH" | cut-cdn -ua -t 50 -q -o "$NO_CDN_FILE"
          LINES_NO_CDN=$(wc -l < "$NO_CDN_FILE")
          echo "  $LINES_NO_CDN lines after cut-cdn"

          # 2. Run hakip2host on No-CDN output and filter (NO CHANGES HERE)
          HAKIP2HOST_RAW_FILE="$OUT_DIR/hakip2host_raw_${CH_BASENAME}"
          HAKIP2HOST_FILTERED_FILE="$OUT_DIR/hakip2host_filtered_${CH_BASENAME}"
          echo "Running hakip2host on $NO_CDN_FILE..."
          if [ "$LINES_NO_CDN" -gt 0 ]; then
            mkdir -p "$(dirname "$HAKIP2HOST_RAW_FILE")"
            cat "$NO_CDN_FILE" | hakip2host | sort -u > "$HAKIP2HOST_RAW_FILE"
            echo "Filtering hakip2host output..."
            grep -Ev 'amazonaws\.com|aws\.com|\.aws|wordpress\.com' "$HAKIP2HOST_RAW_FILE" \
              | awk -F'[[:space:]]' '{print $3}' | sort -u > "$HAKIP2HOST_FILTERED_FILE"
          else
            touch "$HAKIP2HOST_FILTERED_FILE"
          fi
          LINES_HAKIP2HOST=$(wc -l < "$HAKIP2HOST_FILTERED_FILE")
          echo "  $LINES_HAKIP2HOST unique domains after hakip2host filtering"

          # 3. Run Caduceus on original IP chunk (NO CHANGES HERE)
          CADUCEUS_FILE="$OUT_DIR/caduceus_${CH_BASENAME}"
          echo "Running Caduceus on $CH_PATH..."
          caduceus -i "$CH_PATH" | sort -u > "$CADUCEUS_FILE"
          LINES_CADUCEUS=$(wc -l < "$CADUCEUS_FILE")
          echo "  $LINES_CADUCEUS lines after Caduceus"

          # 4. Create a combined, unique target list for httpx (NO CHANGES HERE)
          COMBINED_TARGETS_FILE="$OUT_DIR/combined_targets_${CH_BASENAME}"
          echo "Combining hakip2host and caduceus outputs for httpx..."
          cat "$HAKIP2HOST_FILTERED_FILE" "$CADUCEUS_FILE" | sort -u > "$COMBINED_TARGETS_FILE"
          LINES_COMBINED=$(wc -l < "$COMBINED_TARGETS_FILE")
          echo "  $LINES_COMBINED unique targets for httpx"
          
          # --- vvvvvvvvvvvvvvvv START OF CHANGES vvvvvvvvvvvvvvvv ---

          # 5. Run httpx on the combined target list
          HTTPX_OUT_DIR="results/httpx_out"
          mkdir -p "$HTTPX_OUT_DIR"
          # CHANGE: Using a more consistent base name for all output files
          HTTPX_OUTPUT_PREFIX="${HTTPX_OUT_DIR}/httpx_results_${CH_BASENAME%.txt}"
          HTTPX_FINAL_OUTPUT_PATH="${HTTPX_OUTPUT_PREFIX}.txt"

          echo "Running httpx on $COMBINED_TARGETS_FILE — $LINES_COMBINED lines"
          # Add a debug step to ensure the input file is not empty
          echo "--- START of httpx input file ---"
          head -n 10 "$COMBINED_TARGETS_FILE"
          echo "--- END of httpx input file ---"

          if [ "$LINES_COMBINED" -le 1500 ]; then
            # small: single httpx run
            echo "— ≤1500 lines: single httpx with 100 threads"
            # CHANGE: Removed -silent, added -status-code and -no-color to capture ALL results
            httpx -l "$COMBINED_TARGETS_FILE" -threads 100 -status-code -no-color \
              -o "${HTTPX_FINAL_OUTPUT_PATH}" 
          else
            # big: split into sub-chunks and run with GNU Parallel
            TMP=$(mktemp -d)
            split -l 1500 --numeric-suffixes=1 --suffix-length=2 \
                  --additional-suffix=.txt \
                  "$COMBINED_TARGETS_FILE" "$TMP/sub_"

            if [ "$LINES_COMBINED" -gt 35000 ]; then JOBS=12; THREADS=250
            elif [ "$LINES_COMBINED" -gt 15000 ]; then JOBS=10; THREADS=200
            else JOBS=8; THREADS=100
            fi

            echo "— Launching parallel -j$JOBS with $THREADS threads"
            parallel -j"$JOBS" \
              # CHANGE: Removed -silent, added -status-code and -no-color to capture ALL results
              httpx -l {} -threads "$THREADS" -retries 1 -random-agent -timeout 10 -status-code -no-color -silent \
                    # CHANGE: Simplified and unified the output file naming
                    -o "${HTTPX_OUTPUT_PREFIX}_{/.}.txt"
              ::: "$TMP"/sub_*.txt
            rm -rf "$TMP"
          fi
          
          # For debugging: List the files created by httpx before consolidation
          echo "Listing raw httpx output files before consolidation:"
          ls -l "$HTTPX_OUT_DIR"

          # Consolidate httpx results for this chunk
          # CHANGE: Drastically simplified the 'find' command to use our new unified prefix
          find "$HTTPX_OUT_DIR" -maxdepth 1 -type f -name "httpx_results_${CH_BASENAME%.txt}*.txt" -print0 \
            | xargs -0 --no-run-if-empty cat \
            | sort -u > "${HTTPX_FINAL_OUTPUT_PATH}.tmp" && mv "${HTTPX_FINAL_OUTPUT_PATH}.tmp" "${HTTPX_FINAL_OUTPUT_PATH}"

          # Cleanup intermediate sub-chunk results
          # CHANGE: Simplified the cleanup command to match the new naming scheme
          find "$HTTPX_OUT_DIR" -maxdepth 1 -type f -name "httpx_results_${CH_BASENAME%.txt}_sub_*.txt" -delete

      - name: Prepare artifact for upload
        id: art_details
        run: |
          CH_BASENAME=$(basename "${{ matrix.item.chunk }}")
          ARTIFACT_NAME="results-${CH_BASENAME%.txt}" # e.g., results-chunk_01
          ARTIFACT_DIR="staging/${ARTIFACT_NAME}"     # e.g., staging/results-chunk_01

          echo "Preparing artifact directory: ${ARTIFACT_DIR}"
          mkdir -p "$ARTIFACT_DIR"

          # Move result files into the staging directory
          # Using --no-clobber to prevent errors if a file doesn't exist
          mv -n "results/processed_out/hakip2host_filtered_${CH_BASENAME}" "$ARTIFACT_DIR/"
          mv -n "results/processed_out/caduceus_${CH_BASENAME}" "$ARTIFACT_DIR/"
          mv -n "results/httpx_out/httpx_results_${CH_BASENAME}" "$ARTIFACT_DIR/"
          
          echo "artifact_name=${ARTIFACT_NAME}" >> $GITHUB_OUTPUT
          echo "artifact_path=${ARTIFACT_DIR}" >> $GITHUB_OUTPUT

      - name: Upload per-chunk results directory
        uses: actions/upload-artifact@v4
        with:
          name: ${{ steps.art_details.outputs.artifact_name }}
          path: ${{ steps.art_details.outputs.artifact_path }}
          if-no-files-found: ignore
          retention-days: 1


  aggregate_results:
    needs: [prepare_matrix, httpx]
    runs-on: ubuntu-latest
    if: ${{ needs.prepare_matrix.outputs.matrix != '[]' }}
    steps:
      - name: Download all per-chunk result directories
        uses: actions/download-artifact@v4
        with:
          path: temp-results
          pattern: results-chunk_*
          # CRITICAL FIX: The line 'merge-multiple: true' has been removed.
          # This will now create a directory for each artifact, like:
          # temp-results/results-chunk_01/
          # temp-results/results-chunk_02/
          
      # HELPFUL DEBUGGING STEP: This will show you the exact directory structure.
      - name: List downloaded files for verification
        run: |
          echo "Verifying downloaded artifact structure:"
          ls -R temp-results

      - name: Aggregate all chunk results
        run: |
          

          # Aggregating hakip2host filtered results
          HAKIP2HOST_FINAL_FILE="final_hakip2host_filtered_results.txt"
          echo "Aggregating hakip2host filtered results into $HAKIP2HOST_FINAL_FILE..."
          
          # FIX: Using a more robust wildcard pattern to find the file.
          # This looks for any file that CONTAINS "hakip2host-filtered" and ends in ".txt".
          
          find temp-results -name "*hakip2host-filtered*.txt" -print0 | xargs -0 --no-run-if-empty cat | sort -u > "$HAKIP2HOST_FINAL_FILE"
          echo "Aggregation complete. Total unique hakip2host findings: $(wc -l < "$HAKIP2HOST_FINAL_FILE")"

          # Aggregating caduceus results
          
          CADUCEUS_FINAL_FILE="final_caduceus_results.txt"
          echo "Aggregating caduceus results into $CADUCEUS_FINAL_FILE..."
          # FIX: Applying the same robust pattern for consistency.
          
          find temp-results -name "*caduceus*.txt" -print0 | xargs -0 --no-run-if-empty cat | sort -u > "$CADUCEUS_FINAL_FILE"
          echo "Aggregation complete. Total unique caduceus findings: $(wc -l < "$CADUCEUS_FINAL_FILE")"

          # Aggregating httpx results
          
          HTTPX_FINAL_FILE="final_httpx_results.txt"
          echo "Aggregating httpx results into $HTTPX_FINAL_FILE..."
          
          # FIX: Applying the same robust pattern for consistency.
          
          find temp-results -name "*httpx_results*.txt" -print0 | xargs -0 --no-run-if-empty cat | sort -u > "$HTTPX_FINAL_FILE"
          echo "Aggregation complete. Total unique httpx findings: $(wc -l < "$HTTPX_FINAL_FILE")"


      - name: Upload final aggregated results
        uses: actions/upload-artifact@v4
        with:
          name: final-aggregated-results
          path: |
            final_hakip2host_filtered_results.txt
            final_caduceus_results.txt
            final_httpx_results.txt
          retention-days: 1
